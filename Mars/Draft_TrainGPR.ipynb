{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac561412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from   sklearn.gaussian_process         import GaussianProcessRegressor\n",
    "from   sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from   scipy.spatial.distance           import cdist\n",
    "from   sklearn.preprocessing            import MinMaxScaler, RobustScaler\n",
    "\n",
    "rndm_no = 42\n",
    "np.random.seed(rndm_no) #set numpy random seed\n",
    "R_m = 3389.5 #volumetric mean radius\n",
    "maxRescale = 100\n",
    "sampSize = 28 #int(np.quantile(jh_all.groupby(by = 'orb')['orb'].count(), 0.25))\n",
    "\n",
    "lowAlp  = 1e-10\n",
    "midAlp  = 1e-5\n",
    "highAlp = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://naif.jpl.nasa.gov/pub/naif/MAVEN/kernels/spk/\n",
    "\n",
    "baseURL = 'https://naif.jpl.nasa.gov/pub/naif/MAVEN/kernels/spk/'\n",
    "\n",
    "r = requests.get(baseURL)\n",
    "\n",
    "x = re.findall('\"(maven_orb_rec_.*\\.orb)\"', r.text)\n",
    "\n",
    "x.sort()\n",
    "\n",
    "#os.mkdir('./tempFiles_orb/'), watch out, spaghetti code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c27ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbs      = np.zeros(0)\n",
    "apoDates = np.zeros(0)\n",
    "\n",
    "\n",
    "for link in x:\n",
    "    \n",
    "    r = requests.get(baseURL+link)\n",
    "    \n",
    "    for line in r.text.splitlines()[2:]:\n",
    "        \n",
    "        \n",
    "        orb = line.split()[0]\n",
    "        \n",
    "        dateStr = line.split()[6] + line.split()[7]  + line.split()[8]  + '-' + line.split()[9]\n",
    "        \n",
    "        apoDate = dt.datetime.strptime(dateStr, \"%Y%b%d-%H:%M:%S\")\n",
    "        \n",
    "        orbs = np.append(orbs, np.int32(orb))\n",
    "        \n",
    "        apoDates = np.append(apoDates, apoDate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d89334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d32ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get JH file\n",
    "\n",
    "jh_file = 'https://homepage.physics.uiowa.edu/~jhalekas/drivers/drivers_merge_l2_hires.txt'\n",
    "\n",
    "r = requests.get(jh_file)\n",
    "\n",
    "with open(\"./tempFiles_orb/drivers_merge_l2_hires.txt\", \"wb\") as f:\n",
    "    \n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28d5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0a4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482a37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Halekas file (direct measurements)\n",
    "\n",
    "\n",
    "colNames = ['date', 'np_SW', 'nalpha_SW', 'v_SW', \n",
    "            'v_x_SW', 'v_y_SW', 'v_z_SW', 'tp_SW', \n",
    "            'b_x_SW', 'b_y_SW', 'b_z_SW']\n",
    "\n",
    "jh = pd.read_csv(jh_file, names = colNames, index_col = False, sep = '\\s+')\n",
    "\n",
    "jh['date_SW'] = pd.to_datetime(jh['date'])\n",
    "\n",
    "jh['date_SW_unix'] = (jh['date_SW'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "jh['b_mag_SW'] = np.sqrt(jh.b_x_SW**2.0 + jh.b_y_SW**2.0 + jh.b_z_SW**2.0)\n",
    "\n",
    "jh['v_mag_SW'] = np.sqrt(jh.v_x_SW**2.0 + jh.v_y_SW**2.0 + jh.v_z_SW**2.0)\n",
    "\n",
    "\n",
    "#jh['orb'] = np.nan\n",
    "\n",
    "#jh_subset   = jh[(jh.date_SW >= startDate) & (jh.date_SW < stopDate)].copy()\n",
    "\n",
    "#for i, row in jh_subset.iterrows():\n",
    "\n",
    "#    jh_subset.loc[i, 'date_SW_unix'] = row.date_SW.timestamp()\n",
    "    \n",
    "    #which orbit\n",
    "    \n",
    "#    jh_subset.loc[i, 'orb'] = orbs[row.date_SW <= apoDates][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55df11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log scale the proton temperature and density\n",
    "\n",
    "jh['ln_np_SW']     = np.log(jh['np_SW'])\n",
    "jh['ln_tp_SW']     = np.log(jh['tp_SW'])\n",
    "jh['ln_v_x_SW']    = np.log(-1*jh['v_x_SW'])\n",
    "jh['ln_v_mag_SW']  = np.log(jh['v_mag_SW'])\n",
    "jh['ln_b_mag_SW']  = np.log(jh['b_mag_SW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e591943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params      = ['b_x_SW',     'b_y_SW',    'b_z_SW',   'ln_b_mag_SW',\n",
    "               'ln_v_x_SW',  'v_y_SW',    'v_z_SW',   'ln_v_mag_SW', \n",
    "               'ln_tp_SW',   'ln_np_SW']\n",
    "\n",
    "shortParams = ['b_x_SW',     'b_y_SW',    'b_z_SW',   'b_mag_SW',\n",
    "               '-v_x_SW',     'v_y_SW',    'v_z_SW',   'v_mag_SW', \n",
    "               'tp_SW',      'np_SW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652bb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef6d8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c39e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.0 257.0\n"
     ]
    }
   ],
   "source": [
    "#find which orbits we need to run this over\n",
    "\n",
    "#this denotes where the file runs over the entire time\n",
    "\n",
    "startDate = dt.datetime(2014, 11, 15)\n",
    "stopDate = dt.datetime(2014,  11, 16)\n",
    "\n",
    "orbStartLoop = orbs[apoDates >= startDate][0]\n",
    "\n",
    "orbEndLoop   = orbs[apoDates >= stopDate][0]\n",
    "\n",
    "print(orbStartLoop, orbEndLoop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffe797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fdb4f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only currently set up for hourly to save sanity for testing\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results['date_[utc]']  = pd.to_datetime(np.arange(startDate,              \n",
    "                                                  stopDate,             \n",
    "                                                  dt.timedelta(hours = 1)))\n",
    "\n",
    "results['date_[unix]']  = (results['date_[utc]'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "results['orb']         = np.nan\n",
    "\n",
    "for sp in shortParams:\n",
    "\n",
    "    results['mu_{}'.format(sp)]            = np.nan\n",
    "\n",
    "    results['sigma_{}'.format(sp)]         = np.nan\n",
    "    \n",
    "    results['mu_{}_normed'.format(sp)]     = np.nan\n",
    "\n",
    "    results['sigma_{}_normed'.format(sp)]  = np.nan\n",
    "    \n",
    "    results['flag_{}'.format(sp)]          = np.zeros(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa3ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e5207be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from Orbit: 251.0 to 257.0\n",
      "0 251.0\n",
      "1 252.0\n",
      "2 253.0\n",
      "3 254.0\n",
      "4 255.0\n",
      "5 256.0\n",
      "CPU times: user 6.45 s, sys: 41.4 ms, total: 6.49 s\n",
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#can only go from 1 to -1, this is a little weird for arbitrary dates, will have an edge issue\n",
    "#will want to fix this for future iterations\n",
    "\n",
    "#user puts in date, and buffer on either side instead on the search function from jh_all\n",
    "\n",
    "print('Running from Orbit: {} to {}'.format(orbStartLoop, orbEndLoop))\n",
    "\n",
    "for i, o in enumerate(np.arange(orbStartLoop, orbEndLoop, 1)):\n",
    "    \n",
    "    print(i, o)\n",
    "    \n",
    "    endOrb     = apoDates[orbs == o][0]\n",
    "    startOrb   = apoDates[orbs == o - 1][0]\n",
    "    midOrb     = startOrb + ((endOrb - startOrb) / 2.0)\n",
    "    \n",
    "    #print(startOrb, endOrb)\n",
    "    \n",
    "    #get index for table of output\n",
    "    indResults = ((results['date_[utc]'] >= startOrb) & (results['date_[utc]'] < endOrb))\n",
    "\n",
    "    results.loc[indResults, 'orb'] = o\n",
    "\n",
    "    #what do we actually want. EITHER the orbit if it exists +/- 2 - do this first.\n",
    "    \n",
    "    indJH = ((jh['date_SW'] >= startOrb) & (jh['date_SW'] < endOrb))\n",
    "    \n",
    "    if (sum(indJH) > 0):\n",
    "        \n",
    "        #print(\"Within the dataframe\")\n",
    "    \n",
    "        startIndex = jh[indJH].index[0]  - 2\n",
    "        endIndex   = jh[indJH].index[-1] + 2 \n",
    "        \n",
    "        \n",
    "        #print(startIndex, endIndex)\n",
    "\n",
    "        orbIndex = ((jh.index >= startIndex) & (jh.index <= endIndex))\n",
    "        \n",
    "\n",
    "        #if orbital sample too small?\n",
    "        if sum(orbIndex) < sampSize:\n",
    "        \n",
    "            newDelta = int((sampSize - sum(orbIndex)) / 2.0)\n",
    "            \n",
    "            #print(newDelta)\n",
    "        \n",
    "            startIndex = jh[orbIndex].index[0]  - newDelta\n",
    "            endIndex   = jh[orbIndex].index[-1] + newDelta\n",
    "            \n",
    "            orbIndex = ((jh.index >= startIndex) & (jh.index <= endIndex)) \n",
    "            \n",
    "            #print(sum(orbIndex))\n",
    "            \n",
    "    #or the nearest X points to that actual date...\n",
    "\n",
    "    else:\n",
    "        \n",
    "        #amount on either side that scales to the gap distance\n",
    "        \n",
    "        #closest index to the gap\n",
    "        indEnd   = jh.index[((jh.date_SW - endOrb) >= dt.timedelta(0))][0]\n",
    "        indStart = indEnd - 1\n",
    "        \n",
    "        gapStart = jh.loc[indStart, 'date_SW']\n",
    "        gapEnd   = jh.loc[indEnd, 'date_SW']\n",
    "        \n",
    "        fracLoc = ((midOrb - gapStart) / (gapEnd - gapStart))\n",
    "        \n",
    "        \n",
    "        \n",
    "        newDeltaLeft  = int((1.0 - fracLoc)*sampSize)\n",
    "        newDeltaRight = int(fracLoc*sampSize)\n",
    "        \n",
    "        #print(newDeltaLeft, newDeltaRight)\n",
    "        startIndex = indStart  - newDeltaLeft\n",
    "        endIndex   = indEnd    + newDeltaRight\n",
    "        \n",
    "        orbIndex = ((jh.index >= startIndex) & (jh.index <= endIndex)) \n",
    "        \n",
    "    \n",
    "    data = jh[orbIndex].copy()\n",
    "    \n",
    "\n",
    "    for p, sp in zip(params, shortParams):\n",
    "\n",
    "        X = data.date_SW_unix.values.reshape(-1, 1)  \n",
    "        y = data['{}'.format(p)].values.reshape(-1, 1)\n",
    "\n",
    "        normScaler = RobustScaler()\n",
    "\n",
    "        normScaler.fit(y)\n",
    "\n",
    "        mmScaler = MinMaxScaler(feature_range=(0, maxRescale))\n",
    "        mmScaler.fit(X)\n",
    "\n",
    "        X_normed = mmScaler.transform(X)#transform to reasonable X scale to match our prior GP guess\n",
    "        y_normed = (normScaler.transform(y))[:, 0] #reshape for ML input\n",
    "\n",
    "        dists        = cdist(X_normed, X_normed)\n",
    "\n",
    "        dists_noZeros = dists[dists != 0]\n",
    "\n",
    "        minLength = np.quantile(dists_noZeros, 0.0)\n",
    "\n",
    "        midLength = np.quantile(dists_noZeros, 0.25) #this is hyperparameter\n",
    "\n",
    "        maxLength = np.quantile(dists_noZeros, 0.75)\n",
    "\n",
    "        newScale = np.var(y_normed)#np.max(abs(y_normed))\n",
    "\n",
    "\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            kernelChoice = newScale**2.0 * RationalQuadratic(length_scale = midLength, \n",
    "                                                    length_scale_bounds = (minLength, maxLength), \n",
    "                                                    alpha = midAlp, \n",
    "                                                    alpha_bounds = (lowAlp, highAlp))\n",
    "\n",
    "\n",
    "            gpr = GaussianProcessRegressor(kernel = kernelChoice, random_state = rndm_no)\n",
    "\n",
    "\n",
    "            gpr.fit(X_normed, y_normed)\n",
    "\n",
    "\n",
    "            if w:\n",
    "                \n",
    "                results.loc[indResults, 'flag_{}'.format(sp)]  = np.ones(sum(indResults))\n",
    "\n",
    "                #for warning in w:\n",
    "\n",
    "                    #print(warning.message)\n",
    "\n",
    "                    #print('what')\n",
    "                \n",
    "                \n",
    "                    \n",
    "        #print(p)\n",
    "\n",
    "        #print(gpr.kernel_)\n",
    "            \n",
    "\n",
    "        X_predict = mmScaler.transform(results.loc[indResults, 'date_[unix]'].values.reshape(-1, 1))\n",
    "\n",
    "        mean_predict, std_predict = gpr.predict(X_predict, return_std = True)\n",
    "\n",
    "\n",
    "        '''\n",
    "        X_model   = np.linspace(0, maxRescale, 2000).reshape(-1, 1)\n",
    "        mean_model, std_model = gpr.predict(X_model, return_std = True)\n",
    "        \n",
    "        plt.fill_between(X_model[:, 0], mean_model  - std_model, \n",
    "                                    mean_model  + std_model, \n",
    "                                    color = 'grey', alpha = 0.5)\n",
    "\n",
    "        plt.scatter(X_normed[:, 0], y_normed, color = 'k')\n",
    "\n",
    "        plt.scatter(X_predict[:, 0], mean_predict, color = 'r')\n",
    "\n",
    "        plt.plot(X_model[:, 0], mean_model)\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        '''\n",
    "        results.loc[indResults, 'mu_{}_normed'.format(sp)]     = mean_predict\n",
    "\n",
    "        results.loc[indResults, 'sigma_{}_normed'.format(sp)]  = std_predict\n",
    "            \n",
    "\n",
    "        if 'ln' in p:\n",
    "\n",
    "            results.loc[indResults, 'mu_{}'.format(sp)]         = np.e**normScaler.inverse_transform(mean_predict.reshape(-1, 1))[:, 0]\n",
    "\n",
    "            err_x                                               = std_predict*normScaler.scale_\n",
    "\n",
    "            partial_dqdx                                        = np.e**normScaler.inverse_transform(mean_predict.reshape(-1, 1))[:, 0]\n",
    "\n",
    "            results.loc[indResults, 'sigma_{}'.format(sp)]      = partial_dqdx*err_x\n",
    "\n",
    "        else:\n",
    "\n",
    "            results.loc[indResults, 'mu_{}'.format(sp)] = normScaler.inverse_transform(mean_predict.reshape(-1, 1))[:, 0]\n",
    "\n",
    "            results.loc[indResults, 'sigma_{}'.format(sp)]  = std_predict*normScaler.scale_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d4248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce81ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba73ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6633f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37b066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
